{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385b1bdc-0a84-42e0-ba52-59998a92a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/junshao/bootcamp_Jun_Shao/homework/hw5/notebooks\n",
      "Raw directory exists: True\n",
      "Processed directory exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure project root in sys.path\n",
    "sys.path.append('/Users/junshao/bootcamp_Jun_Shao')\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables\n",
    "data_dir_raw = os.getenv('DATA_DIR_RAW')\n",
    "data_dir_processed = os.getenv('DATA_DIR_PROCESSED')\n",
    "if not all([data_dir_raw, data_dir_processed]):\n",
    "    raise ValueError('DATA_DIR_RAW or DATA_DIR_PROCESSED not set in .env')\n",
    "\n",
    "# Define absolute paths\n",
    "base_dir = '/Users/junshao/bootcamp_Jun_Shao'\n",
    "raw_dir = os.path.join(base_dir, data_dir_raw)\n",
    "processed_dir = os.path.join(base_dir, data_dir_processed)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Verify directories\n",
    "print('Current directory:', os.getcwd())\n",
    "print(f'Raw directory exists: {os.path.exists(raw_dir)}')\n",
    "print(f'Processed directory exists: {os.path.exists(processed_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f210fa76-36b4-4d92-9847-371a2acb5afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Columns: ['date', 'open', 'high', 'low', 'close', 'volume']\n",
      "Input Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158 entries, 0 to 157\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    158 non-null    datetime64[ns]\n",
      " 1   open    158 non-null    float64       \n",
      " 2   high    158 non-null    float64       \n",
      " 3   low     158 non-null    float64       \n",
      " 4   close   158 non-null    float64       \n",
      " 5   volume  158 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1)\n",
      "memory usage: 7.5 KB\n",
      "None\n",
      "Input Shape: (158, 6)\n"
     ]
    }
   ],
   "source": [
    "# Specify input file\n",
    "input_file = '/Users/junshao/bootcamp_Jun_Shao/homework/hw5/data/raw/api_alphavantage_AAPL_20250820-1804.csv'\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f'Input file {input_file} not found')\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure correct dtypes\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)\n",
    "\n",
    "# Validate input\n",
    "print('Input Data Columns:', df.columns.tolist())\n",
    "print('Input Data Info:')\n",
    "print(df.info())\n",
    "print('Input Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a65429c1-d823-4f9d-8870-f0677eb672ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape before saving: (158, 6)\n",
      "DataFrame columns: ['date', 'open', 'high', 'low', 'close', 'volume']\n",
      "Saved CSV to /Users/junshao/bootcamp_Jun_Shao/homework/hw5/data/raw/api_alphavantage_AAPL_20250820-1804.csv\n",
      "CSV file exists: True\n",
      "Saved Parquet to /Users/junshao/bootcamp_Jun_Shao/homework/hw5/data/processed/api_alphavantage_AAPL_20250820-1804.parquet\n",
      "Parquet file exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 构造目录路径\n",
    "raw_dir = '/Users/junshao/bootcamp_Jun_Shao/homework/hw5/data/raw'\n",
    "processed_dir = '/Users/junshao/bootcamp_Jun_Shao/homework/hw5/data/processed'\n",
    "\n",
    "# 构造完整的文件名\n",
    "timestamp = '1804'  # 与输入文件的时间戳一致\n",
    "csv_filename = os.path.join(raw_dir, f'api_alphavantage_AAPL_20250820-{timestamp}.csv')\n",
    "parquet_filename = os.path.join(processed_dir, f'api_alphavantage_AAPL_20250820-{timestamp}.parquet')\n",
    "\n",
    "# 验证 DataFrame\n",
    "print('DataFrame shape before saving:', df.shape)\n",
    "print('DataFrame columns:', df.columns.tolist())\n",
    "\n",
    "# 保存为 CSV\n",
    "try:\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f'Saved CSV to {csv_filename}')\n",
    "    print(f'CSV file exists: {os.path.exists(csv_filename)}')\n",
    "except Exception as e:\n",
    "    print(f'Error saving CSV to {csv_filename}: {e}')\n",
    "\n",
    "# 保存为 Parquet\n",
    "try:\n",
    "    df.to_parquet(parquet_filename, engine='pyarrow', index=False)\n",
    "    print(f'Saved Parquet to {parquet_filename}')\n",
    "    print(f'Parquet file exists: {os.path.exists(parquet_filename)}')\n",
    "except ImportError:\n",
    "    print(f'Error: pyarrow not installed. Install with `pip install pyarrow`.')\n",
    "except Exception as e:\n",
    "    print(f'Error saving Parquet to {parquet_filename}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f6f0e34-4ca2-482b-97ba-db5a066ac343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "All validations passed!\n"
     ]
    }
   ],
   "source": [
    "# Reload files\n",
    "df_csv = pd.read_csv(csv_filename)\n",
    "df_csv['date'] = pd.to_datetime(df_csv['date'])  # Convert date to datetime64[ns]\n",
    "try:\n",
    "    df_parquet = pd.read_parquet(parquet_filename, engine='pyarrow')\n",
    "except ImportError:\n",
    "    print('Error: pyarrow not installed. Install with `pip install pyarrow`.')\n",
    "    df_parquet = None\n",
    "\n",
    "# Validation function\n",
    "def validate_dataframes(df_original, df_csv, df_parquet):\n",
    "    \"\"\"Validate reloaded DataFrames against original.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check shapes\n",
    "    if df_csv.shape != df_original.shape:\n",
    "        errors.append(f'CSV shape {df_csv.shape} does not match original {df_original.shape}')\n",
    "    if df_parquet is not None and df_parquet.shape != df_original.shape:\n",
    "        errors.append(f'Parquet shape {df_parquet.shape} does not match original {df_original.shape}')\n",
    "    \n",
    "    # Check critical columns' dtypes\n",
    "    critical_columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    expected_dtypes = {'date': 'datetime64[ns]', 'open': 'float64', 'high': 'float64', \n",
    "                       'low': 'float64', 'close': 'float64', 'volume': 'int64'}\n",
    "    \n",
    "    for col in critical_columns:\n",
    "        if col not in df_csv.columns:\n",
    "            errors.append(f'CSV missing column: {col}')\n",
    "        elif df_csv[col].dtype != expected_dtypes[col]:\n",
    "            errors.append(f'CSV column {col} dtype {df_csv[col].dtype} does not match expected {expected_dtypes[col]}')\n",
    "        if df_parquet is not None:\n",
    "            if col not in df_parquet.columns:\n",
    "                errors.append(f'Parquet missing column: {col}')\n",
    "            elif df_parquet[col].dtype != expected_dtypes[col]:\n",
    "                errors.append(f'Parquet column {col} dtype {df_parquet[col].dtype} does not match expected {expected_dtypes[col]}')\n",
    "    \n",
    "    return errors\n",
    "\n",
    "# Run validation\n",
    "errors = validate_dataframes(df, df_csv, df_parquet)\n",
    "print('Validation Results:')\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(f'- {error}')\n",
    "else:\n",
    "    print('All validations passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2f4db5e-1a92-4958-8253-0187358dfbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/raw/api_alphavantage_AAPL_20250820-1804.csv\n",
      "Saved ../data/processed/api_alphavantage_AAPL_20250820-1804.parquet\n",
      "Utility Validation Results:\n",
      "All utility validations passed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义 write_df 和 read_df 函数（保持不变）\n",
    "def write_df(df, filename, engine='pyarrow'):\n",
    "    \"\"\"Write DataFrame to CSV or Parquet based on file suffix.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    if filename.endswith('.csv'):\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f'Saved {filename}')\n",
    "    elif filename.endswith('.parquet'):\n",
    "        try:\n",
    "            df.to_parquet(filename, engine=engine, index=False)\n",
    "            print(f'Saved {filename}')\n",
    "        except ImportError:\n",
    "            print(f'Error: Cannot save {filename}. Install pyarrow with `pip install pyarrow`.')\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file suffix in {filename}. Use .csv or .parquet')\n",
    "\n",
    "def read_df(filename, engine='pyarrow'):\n",
    "    \"\"\"Read DataFrame from CSV or Parquet based on file suffix.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f'File {filename} does not exist')\n",
    "    \n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])  # Convert date to datetime64[ns]\n",
    "        return df\n",
    "    elif filename.endswith('.parquet'):\n",
    "        try:\n",
    "            return pd.read_parquet(filename, engine=engine)\n",
    "        except ImportError:\n",
    "            print(f'Error: Cannot read {filename}. Install pyarrow with `pip install pyarrow`.')\n",
    "            return None\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file suffix in {filename}. Use .csv or .parquet')\n",
    "\n",
    "# 构造相对路径（基于 notebooks/ 目录）\n",
    "raw_dir = os.path.join('..', 'data', 'raw')\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "\n",
    "# 构造完整的文件名\n",
    "timestamp = '1804'\n",
    "csv_filename = os.path.join(raw_dir, f'api_alphavantage_AAPL_20250820-{timestamp}.csv')\n",
    "parquet_filename = os.path.join(processed_dir, f'api_alphavantage_AAPL_20250820-{timestamp}.parquet')\n",
    "\n",
    "# 测试工具函数\n",
    "write_df(df, csv_filename)\n",
    "write_df(df, parquet_filename, engine='pyarrow')\n",
    "\n",
    "df_csv_util = read_df(csv_filename)\n",
    "df_parquet_util = read_df(parquet_filename, engine='pyarrow')\n",
    "\n",
    "# 验证加载的数据\n",
    "errors_util = validate_dataframes(df, df_csv_util, df_parquet_util)\n",
    "print('Utility Validation Results:')\n",
    "if errors_util:\n",
    "    for error in errors_util:\n",
    "        print(f'- {error}')\n",
    "else:\n",
    "    print('All utility validations passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f425c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
