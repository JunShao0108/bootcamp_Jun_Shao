{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d98282d",
   "metadata": {},
   "source": [
    "# HW3: Python Fundamentals - Housing Price Prediction\n",
    "\n",
    "This notebook performs the required tasks for the assignment step-by-step. Each section includes a detailed explanation of the task, the code implementation, and comments for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe6bdc",
   "metadata": {},
   "source": [
    "## Step 1: NumPy Operations\n",
    "\n",
    "**Detailed Explanation:**\n",
    "- We start by importing NumPy and the time module to measure execution time.\n",
    "- Create a large array using np.arange() to demonstrate operations on a sizable dataset (1,000,000 elements).\n",
    "- Perform element-wise squaring using a traditional Python loop (list comprehension) and measure the time taken.\n",
    "- Then, perform the same operation using NumPy's vectorized np.square() function and measure the time.\n",
    "- Compare the times to show the efficiency of vectorized operations over loops.\n",
    "- Finally, print the first 5 results to verify the operation.\n",
    "\n",
    "This step addresses the requirement to create an array, perform element-wise operations, and compare loop vs. vectorized execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b740a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop time: 0.0453 seconds\n",
      "Vectorized time: 0.0006 seconds\n",
      "First 5 squared values: [ 0  1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Step 1.1: Create a large array for demonstration\n",
    "array = np.arange(1000000)  # Array from 0 to 999999\n",
    "\n",
    "# Step 1.2: Perform squaring using a loop (non-vectorized)\n",
    "start_time = time.time()  # Start timing\n",
    "loop_result = [x**2 for x in array]  # Square each element using list comprehension\n",
    "loop_time = time.time() - start_time  # End timing\n",
    "\n",
    "# Step 1.3: Perform squaring using NumPy vectorization\n",
    "start_time = time.time()  # Start timing\n",
    "vectorized_result = np.square(array)  # Vectorized squaring\n",
    "vectorized_time = time.time() - start_time  # End timing\n",
    "\n",
    "# Step 1.4: Print results and comparison\n",
    "print(f'Loop time: {loop_time:.4f} seconds')  # Display loop execution time\n",
    "print(f'Vectorized time: {vectorized_time:.4f} seconds')  # Display vectorized execution time\n",
    "print(f'First 5 squared values: {vectorized_result[:5]}')  # Verify results with first 5 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5da3f",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loading\n",
    "\n",
    "**Detailed Explanation:**\n",
    "- Import pandas for data handling.\n",
    "- Load the CSV file from the root directory using pd.read_csv(). The path is '../starter_data.csv' relative to the notebooks folder.\n",
    "- Use df.info() to display the dataset structure, including column names, data types, and non-null counts.\n",
    "- Use df.head() to show the first 5 rows for a quick inspection of the data.\n",
    "\n",
    "This step fulfills the requirement to load the provided CSV and inspect it with .info() and .head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb27c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  10 non-null     object\n",
      " 1   value     10 non-null     int64 \n",
      " 2   date      10 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 372.0+ bytes\n",
      "\n",
      "First 5 rows:\n",
      "  category  value      date\n",
      "0        A     10  2025/8/1\n",
      "1        B     15  2025/8/2\n",
      "2        A     12  2025/8/3\n",
      "3        B     18  2025/8/4\n",
      "4        C     25  2025/8/5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 2.1: Load the dataset from the root directory\n",
    "df = pd.read_csv('/Users/junshao/bootcamp_Jun_Shao/homework/hw3/data/starter_data.csv')  # Read the CSV file into a DataFrame\n",
    "\n",
    "# Step 2.2: Inspect the dataset structure\n",
    "print('Dataset Info:')  # Header for info output\n",
    "df.info()  # Display info: columns, types, non-null counts\n",
    "\n",
    "# Step 2.3: Display the first few rows\n",
    "print('\\nFirst 5 rows:')  # Header for head output\n",
    "print(df.head())  # Show first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf65a3",
   "metadata": {},
   "source": [
    "## Step 3: Summary Statistics\n",
    "\n",
    "**Detailed Explanation:**\n",
    "- To import custom utilities, add the project root directory to sys.path, as the notebook runs in 'notebooks/' and utils.py is in '../src/'.\n",
    "- Use os.getcwd() to get the current working directory and navigate to the parent directory (project root).\n",
    "- Import the get_summary_stats function from src/utils.py.\n",
    "- Call the function on the DataFrame to compute summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for numeric columns.\n",
    "- Print the results for verification.\n",
    "\n",
    "This step calculates .describe() for numeric columns using a reusable function, addressing the requirement for summary statistics and reusable functions (bonus: moved to src/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273f29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/junshao/bootcamp_Jun_Shao/homework/hw3\n",
      "src/utils.py exists: True\n",
      "Summary Statistics:\n",
      "           value\n",
      "count  10.000000\n",
      "mean   17.600000\n",
      "std     7.381659\n",
      "min    10.000000\n",
      "25%    12.250000\n",
      "50%    14.500000\n",
      "75%    23.250000\n",
      "max    30.000000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root to sys.path (parent of notebooks/ directory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Move up one directory from notebooks/\n",
    "sys.path.append(project_root)  # Add project root to Python path\n",
    "\n",
    "# Verify the path for debugging\n",
    "print(f'Project root added to sys.path: {project_root}')\n",
    "print(f'src/utils.py exists: {os.path.exists(os.path.join(project_root, \"src\", \"utils.py\"))}')\n",
    "\n",
    "from src.utils import get_summary_stats\n",
    "\n",
    "# Step 3.1: Calculate summary statistics using the reusable function\n",
    "summary_stats = get_summary_stats(df)  # Call the function to get .describe()\n",
    "\n",
    "# Step 3.2: Display the summary statistics\n",
    "print('Summary Statistics:')  # Header for output\n",
    "print(summary_stats)  # Print the DataFrame of statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13319a92",
   "metadata": {},
   "source": [
    "## Step 4: Groupby Aggregation\n",
    "\n",
    "**Detailed Explanation:**\n",
    "- Use df.groupby() to group the data by a categorical column (assumed 'location'; replace if different).\n",
    "- Aggregate on the 'price' column to compute mean and count.\n",
    "- Include a try-except block to handle if the assumed column doesn't exist, printing available columns for easy debugging.\n",
    "- Print the aggregation results.\n",
    "\n",
    "This step performs .groupby() aggregation by category, as required. If 'location' is not present, update based on printed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9cd7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No \"location\" column found. Please replace with a valid categorical column.\n",
      "Available columns: ['category', 'value', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Attempt to group by 'location' and aggregate 'price'\n",
    "try:\n",
    "    groupby_stats = df.groupby('location')['price'].agg(['mean', 'count'])  # Group and aggregate mean and count\n",
    "    print('Groupby Aggregation (by location):')  # Header for output\n",
    "    print(groupby_stats)  # Display the aggregated DataFrame\n",
    "except KeyError:\n",
    "    print('No \"location\" column found. Please replace with a valid categorical column.')  # Error message\n",
    "    print('Available columns:', df.columns.tolist())  # List all columns for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a34d7",
   "metadata": {},
   "source": [
    "## Step 5: Save Outputs\n",
    "\n",
    "**Detailed Explanation:**\n",
    "- Save the summary statistics to a CSV file in the processed data folder using to_csv().\n",
    "- For the bonus task: Import matplotlib, create a bar plot of the average prices from the groupby results.\n",
    "- Customize the plot with title, labels, and layout.\n",
    "- Save the plot as a PNG file.\n",
    "- Handle cases where groupby_stats might not exist due to column errors.\n",
    "\n",
    "This step saves the outputs correctly and includes the bonus plot creation and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb8eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics saved to data/processed/summary.csv\n",
      "Cannot create plot due to missing groupby_stats.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5.1: Save summary statistics to CSV\n",
    "summary_stats.to_csv('../data/processed/summary.csv')  # Export to CSV\n",
    "print('Summary statistics saved to data/processed/summary.csv')  # Confirmation message\n",
    "\n",
    "# Step 5.2: Bonus - Create and save a bar plot\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Set figure size\n",
    "try:\n",
    "    groupby_stats['mean'].plot(kind='bar')  # Plot mean values as bars\n",
    "    plt.title('Average House Price by Location')  # Set title\n",
    "    plt.xlabel('Location')  # Set x-axis label\n",
    "    plt.ylabel('Average Price')  # Set y-axis label\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig('../data/processed/bar_plot.png')  # Save as PNG\n",
    "    print('Bar plot saved to data/processed/bar_plot.png')  # Confirmation message\n",
    "except NameError:\n",
    "    print('Cannot create plot due to missing groupby_stats.')  # Error message if groupby failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
